{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5_/4x_55p7113ddt777k3x8_cw40000gn/T/ipykernel_99426/2481162812.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd #dataframes\n",
      "[nltk_data] Downloading package udhr to /Users/bunetz/nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Packages\n",
    "import pandas as pd #dataframes\n",
    "import numpy as np #for arrays \n",
    "\n",
    "#NLP libraries\n",
    "import nltk \n",
    "from nltk.corpus import udhr #corpora with texts \n",
    "import re #Regular expressions\n",
    "import spacy \n",
    "import es_core_news_sm, ko_core_news_sm, fi_core_news_sm, zh_core_web_sm, ja_core_news_sm, pl_core_news_sm, de_core_news_sm #spacy models\n",
    "\n",
    "nltk.download('udhr')\n",
    "\n",
    "#NLP objects for (as we can't use shortcuts for loading the objects)\n",
    "nlp_es= spacy.load(\"es_core_news_sm\") #Spanish\n",
    "nlp_ko= spacy.load(\"ko_core_news_sm\") #Korean\n",
    "nlp_fi= spacy.load(\"fi_core_news_sm\") #Finnish\n",
    "nlp_zh= spacy.load(\"zh_core_web_sm\") #Chinese\n",
    "nlp_ja= spacy.load(\"ja_core_news_sm\") #Japanese\n",
    "nlp_pl= spacy.load(\"pl_core_news_sm\") #Polish\n",
    "nlp_de= spacy.load(\"de_core_news_sm\") #German\n",
    "\n",
    "#other spacy models for less explored languages \n",
    "from spacy.lang.tr import Turkish\n",
    "nlp_tr= Turkish()\n",
    "from spacy.lang.id import Indonesian\n",
    "nlp_id= Indonesian()\n",
    "from spacy.lang.ar import Arabic\n",
    "nlp_ar= Arabic()\n",
    "from spacy.lang.tl import Tagalog\n",
    "nlp_tl= Tagalog()\n",
    "from spacy.lang.eu import Basque\n",
    "nlp_eu= Basque()\n",
    "from spacy.lang.et import Estonian\n",
    "nlp_et= Estonian()\n",
    "from spacy.lang.kn import Kannada\n",
    "nlp_kn= Kannada()\n",
    "from spacy.lang.yo import Yoruba \n",
    "nlp_yo= Yoruba()\n",
    "from spacy.lang.sk import Slovak\n",
    "nlp_sk= Slovak()\n",
    "from spacy.lang.ms import Malay\n",
    "nlp_ms= Malay()\n",
    "from spacy.lang.ga import Irish\n",
    "nlp_ga= Irish()\n",
    "from spacy.lang.tn import Setswana\n",
    "nlp_tn= Setswana()\n",
    "from spacy.lang.bg import Bulgarian\n",
    "nlp_bg= Bulgarian()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Family</th>\n",
       "      <th>ISO code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Korean</td>\n",
       "      <td>Koreanic</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finnish</td>\n",
       "      <td>Uralic</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Turkish</td>\n",
       "      <td>Turkic</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indonesian</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>zh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tagalog</td>\n",
       "      <td>Afro-Asiatic</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Basque</td>\n",
       "      <td>N/D</td>\n",
       "      <td>eu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Estonian</td>\n",
       "      <td>Uralic</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kannada</td>\n",
       "      <td>Dravian</td>\n",
       "      <td>kn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Yoruba</td>\n",
       "      <td>Atlantic-Congo</td>\n",
       "      <td>yo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Polish</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>German</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bulgarian</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Setswana</td>\n",
       "      <td>Atlantic-Congo</td>\n",
       "      <td>tn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Slovak</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>sk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Malay</td>\n",
       "      <td>Astronesian</td>\n",
       "      <td>ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Irish</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>ga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Language          Family ISO code\n",
       "0      Spanish   Indo-European       es\n",
       "1       Korean        Koreanic       ko\n",
       "2      Finnish          Uralic       fi\n",
       "3      Turkish          Turkic       tr\n",
       "4   Indonesian    Austronesian       id\n",
       "5      Chinese    Sino-Tibetan       zh\n",
       "6     Japanese         Japonic       ja\n",
       "7       Arabic    Austronesian       ar\n",
       "8      Tagalog    Afro-Asiatic       tl\n",
       "9       Basque             N/D       eu\n",
       "10    Estonian          Uralic       et\n",
       "11     Kannada         Dravian       kn\n",
       "12      Yoruba  Atlantic-Congo       yo\n",
       "13      Polish   Indo-European       pl\n",
       "14      German   Indo-European       de\n",
       "15   Bulgarian   Indo-European       bg\n",
       "16    Setswana  Atlantic-Congo       tn\n",
       "17      Slovak   Indo-European       sk\n",
       "18       Malay     Astronesian       ms\n",
       "19       Irish   Indo-European       ga"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"data/language_data.csv\", sep=\";\")\n",
    "languages= df['Language'].values\n",
    "codes= df['ISO code'].values\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_texts(list_of_languages, list_of_codes):\n",
    "  raw_files_names= {}\n",
    "  for language in list_of_languages:\n",
    "    for code in list_of_codes:\n",
    "        all_files= nltk.corpus.udhr.fileids()\n",
    "        file= [f for f in all_files if re.findall(language, f)][0]\n",
    "        file_name= \"../data/\" + str(code) + \".txt\"\n",
    "        raw= nltk.corpus.udhr.raw(file)\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(raw)    \n",
    "        raw_files_names[language]=raw\n",
    "  return raw_files_names\n",
    "\n",
    "def tokenizer(text, model_lang):\n",
    "    nlp= model_lang #Opens spacy object\n",
    "    doc=nlp(text) #Process text with spacy \n",
    "    tokens = [token.text for token in doc if not token.is_space and not token.is_punct and not token.is_digit]\n",
    "    return tokens\n",
    "\n",
    "def tokens(dict_raw_texts, languages): #takes real_tokenizer and filters by language to tokenize\n",
    "    tokens_langs= {} #dictionary to store output\n",
    "    for lang in languages:\n",
    "        if lang == 'Spanish':\n",
    "            text= dict_raw_texts[lang] #gets text from dict in raw_files_names \n",
    "            model_lang= nlp_es #loads corresponding model\n",
    "            tokens= tokenizer(text, model_lang) #tokenizes \n",
    "            tokens_langs[lang]=tokens #appends to output dictionary \n",
    "        elif lang == 'Korean':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ko\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Finnish':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_fi\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Chinese':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_zh\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Japanese':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ja\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Polish':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_pl\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'German':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_de\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Turkish':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_tr\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Indonesian':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_id\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Arabic':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ar\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Tagalog':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_tl\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Basque':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_eu\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Estonian':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_et\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Kannada':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_kn\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Yoruba':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_yo\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Malay':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ms\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Slovak':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_sk\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Setswana':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_tn\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Bulgarian':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_bg\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Irish':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ga\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "    return tokens_langs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts= extract_raw_texts(languages, codes) #returns a dictionary where KEY is language and VALUE a string with raw text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "all_languages_tokens= tokens(raw_texts, languages) #returns a dictionary where KEY is language and VALUE is list with tokens.\n",
    "print(len(all_languages_tokens)) #just for checking how many variables have been processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "def process(tokens):\n",
    "    token_freq = Counter(tokens)\n",
    "    matrix = []\n",
    "    for token in set(tokens):\n",
    "        matrix.append([token, len(token), token_freq[token]])\n",
    "    \n",
    "    matrix.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "tokens_langs= {}\n",
    "for language in all_languages_tokens:\n",
    "    tokens_langs[language] = process(all_languages_tokens[language])\n",
    "\n",
    "def matrix_to_csv(matrix, filename):\n",
    "    with open(filename, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # Write each row of the matrix to the CSV file\n",
    "        writer.writerow(['Token', 'Length', 'Frequency'])\n",
    "        for row in matrix:\n",
    "            writer.writerow(row)\n",
    "\n",
    "for language in tokens_langs:\n",
    "    filename = f\"data/output_{language.lower()}.csv\"\n",
    "    matrix_to_csv(tokens_langs[language], filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
