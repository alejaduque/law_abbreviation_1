{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5_/4x_55p7113ddt777k3x8_cw40000gn/T/ipykernel_24609/3888155014.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd #dataframes\n",
      "[nltk_data] Downloading package udhr to /Users/bunetz/nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Packages\n",
    "import pandas as pd #dataframes\n",
    "import numpy as np #for arrays \n",
    "\n",
    "#NLP libraries\n",
    "import nltk \n",
    "from nltk.corpus import udhr #corpora with texts \n",
    "import re #Regular expressions\n",
    "import spacy \n",
    "import es_core_news_sm, ko_core_news_sm, fi_core_news_sm, zh_core_web_sm, ja_core_news_sm, pl_core_news_sm, de_core_news_sm #spacy models\n",
    "\n",
    "nltk.download('udhr')\n",
    "\n",
    "#NLP objects for (as we can't use shortcuts for loading the objects)\n",
    "nlp_es= spacy.load(\"es_core_news_sm\") #Spanish\n",
    "nlp_ko= spacy.load(\"ko_core_news_sm\") #Korean\n",
    "nlp_fi= spacy.load(\"fi_core_news_sm\") #Finnish\n",
    "nlp_zh= spacy.load(\"zh_core_web_sm\") #Chinese\n",
    "nlp_ja= spacy.load(\"ja_core_news_sm\") #Japanese\n",
    "nlp_pl= spacy.load(\"pl_core_news_sm\") #Polish\n",
    "nlp_de= spacy.load(\"de_core_news_sm\") #German\n",
    "\n",
    "#other spacy models for less explored languages \n",
    "from spacy.lang.tr import Turkish\n",
    "nlp_tr= Turkish()\n",
    "from spacy.lang.id import Indonesian\n",
    "nlp_id= Indonesian()\n",
    "from spacy.lang.ar import Arabic\n",
    "nlp_ar= Arabic()\n",
    "from spacy.lang.tl import Tagalog\n",
    "nlp_tl= Tagalog()\n",
    "from spacy.lang.eu import Basque\n",
    "nlp_eu= Basque()\n",
    "from spacy.lang.et import Estonian\n",
    "nlp_et= Estonian()\n",
    "from spacy.lang.kn import Kannada\n",
    "nlp_kn= Kannada()\n",
    "from spacy.lang.yo import Yoruba \n",
    "nlp_yo= Yoruba()\n",
    "#from spacy.lang.vi import Vietnamese\n",
    "#nlp_vi= Vietnamese()\n",
    "from spacy.lang.ms import Malay\n",
    "nlp_ms= Malay()\n",
    "from spacy.lang.ga import Irish\n",
    "nlp_ga= Irish()\n",
    "from spacy.lang.tn import Setswana\n",
    "nlp_tn= Setswana()\n",
    "from spacy.lang.bg import Bulgarian\n",
    "nlp_bg= Bulgarian()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Family</th>\n",
       "      <th>Tokenizer</th>\n",
       "      <th>Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>es_core_news_sm</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Korean</td>\n",
       "      <td>Koreanic</td>\n",
       "      <td>ko_core_news_sm</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Finnish</td>\n",
       "      <td>Uralic</td>\n",
       "      <td>fi_core_news_sm</td>\n",
       "      <td>fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Turkish</td>\n",
       "      <td>Turkic</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indonesian</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>zh_core_web_sm</td>\n",
       "      <td>zh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>ja_core_news_sm</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tagalog</td>\n",
       "      <td>Afro-Asiatic</td>\n",
       "      <td>Tagalog</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Basque</td>\n",
       "      <td>N/D</td>\n",
       "      <td>Basque</td>\n",
       "      <td>eu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Estonian</td>\n",
       "      <td>Uralic</td>\n",
       "      <td>Estonian</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kannada</td>\n",
       "      <td>Dravian</td>\n",
       "      <td>Kannada</td>\n",
       "      <td>kn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Yoruba</td>\n",
       "      <td>Atlantic-Congo</td>\n",
       "      <td>Yoruba</td>\n",
       "      <td>yo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Polish</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>pl_core_news_sm</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>German</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>de_core_news_sm</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bulgarian</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Bulgarian</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Setswana</td>\n",
       "      <td>Atlantic-Congo</td>\n",
       "      <td>Setswana</td>\n",
       "      <td>tn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>Austroasiatic</td>\n",
       "      <td>Viatnamese</td>\n",
       "      <td>vi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Malay</td>\n",
       "      <td>Astronesian</td>\n",
       "      <td>Malay</td>\n",
       "      <td>ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Irish</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Irish</td>\n",
       "      <td>ga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Language          Family        Tokenizer Code\n",
       "0      Spanish   Indo-European  es_core_news_sm   es\n",
       "1       Korean        Koreanic  ko_core_news_sm   ko\n",
       "2      Finnish          Uralic  fi_core_news_sm   fi\n",
       "3      Turkish          Turkic          Turkish   tr\n",
       "4   Indonesian    Austronesian       Indonesian   id\n",
       "5      Chinese    Sino-Tibetan   zh_core_web_sm   zh\n",
       "6     Japanese         Japonic  ja_core_news_sm   ja\n",
       "7       Arabic    Austronesian           Arabic   ar\n",
       "8      Tagalog    Afro-Asiatic          Tagalog   tl\n",
       "9       Basque             N/D           Basque   eu\n",
       "10    Estonian          Uralic         Estonian   et\n",
       "11     Kannada         Dravian          Kannada   kn\n",
       "12      Yoruba  Atlantic-Congo           Yoruba   yo\n",
       "13      Polish   Indo-European  pl_core_news_sm   pl\n",
       "14      German   Indo-European  de_core_news_sm   de\n",
       "15   Bulgarian   Indo-European        Bulgarian   bg\n",
       "16    Setswana  Atlantic-Congo         Setswana   tn\n",
       "17  Vietnamese   Austroasiatic       Viatnamese   vi\n",
       "18       Malay     Astronesian            Malay   ms\n",
       "19       Irish   Indo-European            Irish   ga"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"language_data.csv\", sep=\";\")\n",
    "languages= df['Language'].values\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_texts(list_of_languages):\n",
    "  raw_files_names= {}\n",
    "  for language in list_of_languages:\n",
    "    all_files= nltk.corpus.udhr.fileids()\n",
    "    file= [f for f in all_files if re.findall(language, f)][0]\n",
    "    raw= nltk.corpus.udhr.raw(file)\n",
    "    raw_files_names[language]=raw\n",
    "  return raw_files_names\n",
    "\n",
    "def tokenizer(text, model_lang):\n",
    "    nlp= model_lang #Opens spacy object\n",
    "    doc=nlp(text) #Process text with spacy \n",
    "    tokens= [] #for storing tokens\n",
    "    tokens = [token.text for token in doc if not token.is_space and not token.is_punct and not token.is_digit]\n",
    "    return tokens\n",
    "\n",
    "def tokens(dict_raw_texts, languages): #takes real_tokenizer and filters by language to tokenize\n",
    "    tokens_langs= {} #dictionary to store output\n",
    "    for lang in languages:\n",
    "        if lang == 'Spanish':\n",
    "            text= dict_raw_texts[lang] #gets text from dict in raw_files_names \n",
    "            model_lang= nlp_es #loads corresponding model\n",
    "            tokens= tokenizer(text, model_lang) #tokenizes \n",
    "            tokens_langs[lang]=tokens #appends to output dictionary \n",
    "        elif lang == 'Korean':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ko\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Finnish':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_fi\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Chinese':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_zh\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Japanese':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ja\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Polish':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_pl\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'German':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_de\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Turkish':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_tr\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Indonesian':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_id\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Arabic':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ar\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Tagalog':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_tl\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Basque':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_eu\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Estonian':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_et\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Kannada':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_kn\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Yoruba':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_yo\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Malay':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ms\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        #elif lang == 'Vietnamese':\n",
    "          #  text= dict_raw_texts[lang]\n",
    "           # model_lang= nlp_vi\n",
    "           # tokens= tokenizer(text, model_lang)\n",
    "           # tokens_langs[lang]=tokens\n",
    "        elif lang == 'Setswana':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_tn\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Bulgarian':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_bg\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Irish':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ga\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "    return tokens_langs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts= extract_raw_texts(languages) #returns a dictionary where KEY is language and VALUE a string with raw text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "all_languages_tokens= tokens(raw_texts, languages) #returns a dictionary where KEY is language and VALUE is list with tokens.\n",
    "print(len(all_languages_tokens)) #just for checking how many variables have been processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "def process(tokens):\n",
    "    token_freq = Counter(tokens)\n",
    "    matrix = []\n",
    "    for token in set(tokens):\n",
    "        matrix.append([token, len(token), token_freq[token]])\n",
    "    \n",
    "    matrix.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "tokens_langs= {}\n",
    "for language in all_languages_tokens:\n",
    "    tokens_langs[language] = process(all_languages_tokens[language])\n",
    "\n",
    "def matrix_to_csv(matrix, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # Write each row of the matrix to the CSV file\n",
    "        writer.writerow(['Token', 'Length', 'Frequency'])\n",
    "        for row in matrix:\n",
    "            writer.writerow(row)\n",
    "\n",
    "for language in tokens_langs:\n",
    "    filename = f\"output_{language.lower()}.csv\"\n",
    "    matrix_to_csv(tokens_langs[language], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
